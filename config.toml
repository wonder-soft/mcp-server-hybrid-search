# mcp-server-hybrid-search configuration

# Qdrant connection (gRPC port)
qdrant_url = "http://localhost:6334"

# Qdrant collection name
# Overridden by --project flag (e.g. --project my-proj sets collection_name = "my-proj")
collection_name = "docs"

# Tantivy index directory
# Overridden by --project flag (e.g. --project my-proj sets tantivy_index_dir = "~/.mcp-hybrid-search/tantivy/my-proj")
# tantivy_index_dir = "~/.mcp-hybrid-search/tantivy"

# Chunk settings
chunk_size = 1000
chunk_overlap = 200

# MCP server listen port
listen_port = 7070

# Embedding settings
# Provider: "openai" (requires OPENAI_API_KEY), "gemini" (requires GEMINI_API_KEY),
#           or "local" (requires --features local-embed)
embedding_provider = "openai"
embedding_model = "text-embedding-3-small"
embedding_dimension = 1536
# For Gemini embedding (768 dim, configurable via output_dimensionality):
#   embedding_provider = "gemini"
#   embedding_model = "gemini-embedding-001"
#   embedding_dimension = 768
# For local embedding with multilingual-e5-base (768 dim, recommended):
#   embedding_provider = "local"
#   embedding_model = "multilingual-e5-base"
#   embedding_dimension = 768
# For local embedding with multilingual-e5-small (384 dim, lighter):
#   embedding_provider = "local"
#   embedding_model = "multilingual-e5-small"
#   embedding_dimension = 384

# Tokenizer for BM25 full-text search
# Options: "default", "japanese", "korean", "chinese"
# Non-default tokenizers require building with the corresponding feature:
#   cargo build --features ja   (for "japanese")
#   cargo build --features ko   (for "korean")
#   cargo build --features zh   (for "chinese")
tokenizer = "default"
